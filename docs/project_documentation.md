# ğŸ“ context-app â€“ project structure

```TEXT
context-app/
â”‚
â”œâ”€â”€ main.py                  # streamlit/gradio app entry point
â”œâ”€â”€ config.py                # model keys, constants, paths
â”œâ”€â”€ .env                     # environment variables
â”œâ”€â”€ requirements.txt         # python dependencies
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ loader.py            # file loading using langchain loaders
â”‚   â””â”€â”€ splitter.py          # chunking using text splitters
â”‚
â”œâ”€â”€ embedding/
â”‚   â”œâ”€â”€ embedder.py          # embedding logic (openai or local)
â”‚   â””â”€â”€ vector_store.py      # faiss vector store creation/loading
â”‚
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ retriever.py         # query handling and top-k retrieval
â”‚
â”œâ”€â”€ generation/
â”‚   â””â”€â”€ rag_chain.py         # prompt formatting + llm call
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ interface.py         # ui layout and user interactions
â”‚
â””â”€â”€ utils/
    â””â”€â”€ helpers.py           # utilities: file renaming, extension checks, etc.
```
---

## ğŸ” file processing pipeline
````
1. loading: load files using langchain loaders
                     |
                     v
2. splitting: chunk files using langchain docs splitter
                     |
                     v
3. embedding: embed chunks using openai embedder
                     |
                     v
4. vector store: create/load faiss vector store
                     |
                     v
5. retrieval: handle queries and retrieve top-k chunks
````

---

